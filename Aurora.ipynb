{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-14T12:10:05.014779Z",
     "iopub.status.busy": "2023-05-14T12:10:05.014324Z",
     "iopub.status.idle": "2023-05-14T12:10:05.022351Z",
     "shell.execute_reply": "2023-05-14T12:10:05.020972Z",
     "shell.execute_reply.started": "2023-05-14T12:10:05.014746Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T09:18:00.224017Z",
     "iopub.status.busy": "2023-05-14T09:18:00.223428Z",
     "iopub.status.idle": "2023-05-14T09:18:00.235243Z",
     "shell.execute_reply": "2023-05-14T09:18:00.233576Z",
     "shell.execute_reply.started": "2023-05-14T09:18:00.223968Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T10:28:34.092068Z",
     "iopub.status.busy": "2023-05-14T10:28:34.090866Z",
     "iopub.status.idle": "2023-05-14T10:28:34.101562Z",
     "shell.execute_reply": "2023-05-14T10:28:34.100285Z",
     "shell.execute_reply.started": "2023-05-14T10:28:34.092018Z"
    }
   },
   "outputs": [],
   "source": [
    "def Read_Resize_Normalize_images(image_path,L):\n",
    "    \n",
    "    if L == 'Au':\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    try:\n",
    "        if image_path != '/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Au/Thumbs.db' or image_path != '/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp/_list.txt' or image_path != '/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp/Thumbs.db':\n",
    "            \n",
    "            img = Image.open(image_path)\n",
    "\n",
    "            new_shape = (32, 32)\n",
    "\n",
    "            resized_image = resize(np.array(img), new_shape, anti_aliasing=True)\n",
    "\n",
    "            # Normalizing\n",
    "            X = np.array(resized_image, dtype=np.float32).transpose(2, 0, 1) / 255.0\n",
    "            \n",
    "            if X.shape == (4,32,32):\n",
    "                print(image_path)\n",
    "\n",
    "            tensor_image = torch.from_numpy(X)\n",
    "\n",
    "            return (tensor_image,label)\n",
    "    \n",
    "    except:\n",
    "        print(image_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T10:28:36.640355Z",
     "iopub.status.busy": "2023-05-14T10:28:36.639835Z",
     "iopub.status.idle": "2023-05-14T10:28:36.655455Z",
     "shell.execute_reply": "2023-05-14T10:28:36.654330Z",
     "shell.execute_reply.started": "2023-05-14T10:28:36.640317Z"
    }
   },
   "outputs": [],
   "source": [
    "authentic = os.listdir('/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Au')\n",
    "tampered = os.listdir('/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T10:28:37.152309Z",
     "iopub.status.busy": "2023-05-14T10:28:37.151176Z",
     "iopub.status.idle": "2023-05-14T10:28:37.214558Z",
     "shell.execute_reply": "2023-05-14T10:28:37.213079Z",
     "shell.execute_reply.started": "2023-05-14T10:28:37.152262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_a =  '/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Au'\n",
    "p_t = '/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp'\n",
    "\n",
    "\n",
    "A = Read_Resize_Normalize_images(p_a+'/'+authentic[1],'Au')\n",
    "A[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T10:28:44.088257Z",
     "iopub.status.busy": "2023-05-14T10:28:44.086927Z",
     "iopub.status.idle": "2023-05-14T10:35:56.835216Z",
     "shell.execute_reply": "2023-05-14T10:35:56.834046Z",
     "shell.execute_reply.started": "2023-05-14T10:28:44.088216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Au/Thumbs.db\n",
      "/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp/_list.txt\n",
      "/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp/Thumbs.db\n",
      "/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2/Tp/Tp_D_NRD_S_B_ani20002_nat20042_02437.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = []\n",
    "\n",
    "# Reading Authentic Images\n",
    "for i in authentic:\n",
    "    path = os.path.join(p_a+ '/'+ i)\n",
    "    A = Read_Resize_Normalize_images(path,'Au')\n",
    "    dataset.append(A)\n",
    "    \n",
    "    \n",
    "\n",
    "# Reading Tampered images\n",
    "imagesT = []   \n",
    "for i in tampered:\n",
    "    path = os.path.join(p_t+ '/'+ i)\n",
    "    A = Read_Resize_Normalize_images(path,'Tp')\n",
    "    dataset.append(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T11:35:41.981023Z",
     "iopub.status.busy": "2023-05-14T11:35:41.980410Z",
     "iopub.status.idle": "2023-05-14T11:35:41.995864Z",
     "shell.execute_reply": "2023-05-14T11:35:41.994667Z",
     "shell.execute_reply.started": "2023-05-14T11:35:41.980984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12617\n",
      "12614\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset = list(filter(lambda data: data is not None, dataset))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T11:35:53.322647Z",
     "iopub.status.busy": "2023-05-14T11:35:53.322236Z",
     "iopub.status.idle": "2023-05-14T11:35:53.342658Z",
     "shell.execute_reply": "2023-05-14T11:35:53.340935Z",
     "shell.execute_reply.started": "2023-05-14T11:35:53.322617Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "## Shuffling the list\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:04.620072Z",
     "iopub.status.busy": "2023-05-14T13:34:04.619566Z",
     "iopub.status.idle": "2023-05-14T13:34:04.634600Z",
     "shell.execute_reply": "2023-05-14T13:34:04.633260Z",
     "shell.execute_reply.started": "2023-05-14T13:34:04.620033Z"
    }
   },
   "outputs": [],
   "source": [
    "class Image_Manipulation_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Image_Manipulation_Model,self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.down_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "        )\n",
    "        \n",
    "        self.down_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "        )\n",
    "        \n",
    "        self.down_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=128*4*4, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            self.relu,\n",
    "            nn.Linear(in_features=1024, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.relu,\n",
    "            nn.Linear(in_features=128, out_features=2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        d1 = self.down_conv1(img)\n",
    "        d2 = self.down_conv2(d1)\n",
    "        d3 = self.down_conv3(d2)\n",
    "        d3 = d3.view(-1, d3.shape[1]*d3.shape[2]*d3.shape[3])\n",
    "        out = self.linear(d3)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:05.622679Z",
     "iopub.status.busy": "2023-05-14T13:34:05.621414Z",
     "iopub.status.idle": "2023-05-14T13:34:05.662762Z",
     "shell.execute_reply": "2023-05-14T13:34:05.661719Z",
     "shell.execute_reply.started": "2023-05-14T13:34:05.622614Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Image_Manipulation_Model()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "opt = optim.Adam(model.parameters(), lr=0.0004)\n",
    "loss_fn1 = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:06.473598Z",
     "iopub.status.busy": "2023-05-14T13:34:06.472942Z",
     "iopub.status.idle": "2023-05-14T13:34:06.482272Z",
     "shell.execute_reply": "2023-05-14T13:34:06.480587Z",
     "shell.execute_reply.started": "2023-05-14T13:34:06.473548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_Manipulation_Model(\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (down_conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down_conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down_conv3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (7): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Parameters:  2325634\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(\"Parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:07.489089Z",
     "iopub.status.busy": "2023-05-14T13:34:07.488650Z",
     "iopub.status.idle": "2023-05-14T13:34:07.650633Z",
     "shell.execute_reply": "2023-05-14T13:34:07.649361Z",
     "shell.execute_reply.started": "2023-05-14T13:34:07.489049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4932, 0.5068],\n",
      "        [0.6436, 0.3564],\n",
      "        [0.6093, 0.3907],\n",
      "        [0.4000, 0.6000],\n",
      "        [0.5843, 0.4157],\n",
      "        [0.4083, 0.5917],\n",
      "        [0.6809, 0.3191],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.3607, 0.6393],\n",
      "        [0.4247, 0.5753],\n",
      "        [0.6374, 0.3626],\n",
      "        [0.4680, 0.5320],\n",
      "        [0.5332, 0.4668],\n",
      "        [0.3676, 0.6324],\n",
      "        [0.3652, 0.6348],\n",
      "        [0.3845, 0.6155],\n",
      "        [0.4017, 0.5983],\n",
      "        [0.4914, 0.5086],\n",
      "        [0.6834, 0.3166],\n",
      "        [0.4912, 0.5088],\n",
      "        [0.4017, 0.5983],\n",
      "        [0.3726, 0.6274],\n",
      "        [0.5049, 0.4951],\n",
      "        [0.5857, 0.4143],\n",
      "        [0.4453, 0.5547],\n",
      "        [0.4453, 0.5547],\n",
      "        [0.3658, 0.6342],\n",
      "        [0.4133, 0.5867],\n",
      "        [0.5795, 0.4205],\n",
      "        [0.6031, 0.3969],\n",
      "        [0.3629, 0.6371],\n",
      "        [0.5823, 0.4177],\n",
      "        [0.4237, 0.5763],\n",
      "        [0.7787, 0.2213],\n",
      "        [0.3793, 0.6207],\n",
      "        [0.4541, 0.5459],\n",
      "        [0.4166, 0.5834],\n",
      "        [0.4295, 0.5705],\n",
      "        [0.3451, 0.6549],\n",
      "        [0.2928, 0.7072],\n",
      "        [0.4352, 0.5648],\n",
      "        [0.3821, 0.6179],\n",
      "        [0.4501, 0.5499],\n",
      "        [0.3906, 0.6094],\n",
      "        [0.5072, 0.4928],\n",
      "        [0.7612, 0.2388],\n",
      "        [0.2899, 0.7101],\n",
      "        [0.4012, 0.5988],\n",
      "        [0.6308, 0.3692],\n",
      "        [0.5773, 0.4227],\n",
      "        [0.6036, 0.3964],\n",
      "        [0.5733, 0.4267],\n",
      "        [0.4804, 0.5196],\n",
      "        [0.3265, 0.6735],\n",
      "        [0.4774, 0.5226],\n",
      "        [0.4283, 0.5717],\n",
      "        [0.3625, 0.6375],\n",
      "        [0.2819, 0.7181],\n",
      "        [0.3790, 0.6210],\n",
      "        [0.5307, 0.4693],\n",
      "        [0.5539, 0.4461],\n",
      "        [0.6143, 0.3857],\n",
      "        [0.5353, 0.4647],\n",
      "        [0.4177, 0.5823],\n",
      "        [0.4820, 0.5180],\n",
      "        [0.4937, 0.5063],\n",
      "        [0.3635, 0.6365],\n",
      "        [0.6471, 0.3529],\n",
      "        [0.4178, 0.5822],\n",
      "        [0.6852, 0.3148],\n",
      "        [0.5473, 0.4527],\n",
      "        [0.3187, 0.6813],\n",
      "        [0.4565, 0.5435],\n",
      "        [0.4310, 0.5690],\n",
      "        [0.3823, 0.6177],\n",
      "        [0.5279, 0.4721],\n",
      "        [0.6007, 0.3993],\n",
      "        [0.3507, 0.6493],\n",
      "        [0.5979, 0.4021],\n",
      "        [0.4315, 0.5685],\n",
      "        [0.4579, 0.5421],\n",
      "        [0.3859, 0.6141],\n",
      "        [0.6937, 0.3063],\n",
      "        [0.3075, 0.6925],\n",
      "        [0.3571, 0.6429],\n",
      "        [0.5996, 0.4004],\n",
      "        [0.3082, 0.6918],\n",
      "        [0.5587, 0.4413],\n",
      "        [0.4285, 0.5715],\n",
      "        [0.4618, 0.5382],\n",
      "        [0.4341, 0.5659],\n",
      "        [0.2396, 0.7604],\n",
      "        [0.4716, 0.5284],\n",
      "        [0.4816, 0.5184],\n",
      "        [0.4556, 0.5444],\n",
      "        [0.4929, 0.5071],\n",
      "        [0.4858, 0.5142],\n",
      "        [0.3644, 0.6356],\n",
      "        [0.3714, 0.6286],\n",
      "        [0.3072, 0.6928],\n",
      "        [0.3329, 0.6671],\n",
      "        [0.3483, 0.6517],\n",
      "        [0.4672, 0.5328],\n",
      "        [0.5237, 0.4763],\n",
      "        [0.3504, 0.6496],\n",
      "        [0.2726, 0.7274],\n",
      "        [0.5372, 0.4628],\n",
      "        [0.5467, 0.4533],\n",
      "        [0.4593, 0.5407],\n",
      "        [0.4660, 0.5340],\n",
      "        [0.5831, 0.4169],\n",
      "        [0.6158, 0.3842],\n",
      "        [0.3795, 0.6205],\n",
      "        [0.5013, 0.4987],\n",
      "        [0.6620, 0.3380],\n",
      "        [0.4788, 0.5212],\n",
      "        [0.3480, 0.6520],\n",
      "        [0.5064, 0.4936],\n",
      "        [0.5919, 0.4081],\n",
      "        [0.5022, 0.4978],\n",
      "        [0.4535, 0.5465],\n",
      "        [0.4652, 0.5348],\n",
      "        [0.4774, 0.5226],\n",
      "        [0.3825, 0.6175],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.2399, 0.7601],\n",
      "        [0.5022, 0.4978],\n",
      "        [0.4477, 0.5523]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create a random image tensor of shape (batch_size, 3, 32, 32)\n",
    "image = torch.randn(128, 3, 32, 32)\n",
    "\n",
    "# create an instance of the Image_Manipulation_Model class\n",
    "model = Image_Manipulation_Model()\n",
    "\n",
    "# pass the image tensor to the model\n",
    "output = model(image)\n",
    "\n",
    "# print the output shape\n",
    "print(output)  # should output torch.Size([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:14.182544Z",
     "iopub.status.busy": "2023-05-14T13:34:14.182020Z",
     "iopub.status.idle": "2023-05-14T13:34:14.192174Z",
     "shell.execute_reply": "2023-05-14T13:34:14.190754Z",
     "shell.execute_reply.started": "2023-05-14T13:34:14.182495Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [8614, 3000, 1000])\n",
    "train_data = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_data = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:15.031817Z",
     "iopub.status.busy": "2023-05-14T13:34:15.031394Z",
     "iopub.status.idle": "2023-05-14T13:34:15.037888Z",
     "shell.execute_reply": "2023-05-14T13:34:15.036464Z",
     "shell.execute_reply.started": "2023-05-14T13:34:15.031781Z"
    }
   },
   "outputs": [],
   "source": [
    "# for x, y in validation_dataset:\n",
    "#     print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:34:15.796678Z",
     "iopub.status.busy": "2023-05-14T13:34:15.796257Z",
     "iopub.status.idle": "2023-05-14T13:36:22.165044Z",
     "shell.execute_reply": "2023-05-14T13:36:22.163784Z",
     "shell.execute_reply.started": "2023-05-14T13:34:15.796646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 \n",
      "|--- Train Loss: 0.6684401035308838\n",
      "|--- Train Loss: 0.6793340146541595\n",
      "|--- Train Loss: 0.6789886554082235\n",
      "|--- Train Loss: 0.6842653751373291\n",
      "|--- Train Loss: 0.6836652040481568\n",
      "|--- Train Loss: 0.6850891709327698\n",
      "|--- Train Loss: 0.6868431653295245\n",
      "|--- Train Loss: 0.68777796626091\n",
      "|--- Train Loss: 0.6873221529854668\n",
      "|--- Train Loss: 0.6885446786880494\n",
      "|--- Train Loss: 0.6895647157322277\n",
      "|--- Train Loss: 0.6893447786569595\n",
      "|--- Train Loss: 0.6890011017139142\n",
      "|--- Train Loss: 0.6890389110360827\n",
      "|--- Train Loss: 0.6898701985677084\n",
      "|--- Train Loss: 0.6894473917782307\n",
      "|--- Train Loss: 0.6904397326357224\n",
      "|--- Train Loss: 0.6904568043020036\n",
      "|--- Train Loss: 0.6902438935480619\n",
      "|--- Train Loss: 0.6899227976799012\n",
      "|--- Train Loss: 0.6898846683048067\n",
      "|--- Train Loss: 0.6898377619006417\n",
      "|--- Train Loss: 0.6896309230638586\n",
      "|--- Train Loss: 0.6890202139814695\n",
      "|--- Train Loss: 0.6885170340538025\n",
      "|--- Train Loss: 0.6881594474499042\n",
      "|--- Train Loss: 0.6883819081165172\n",
      "|--- Train Loss: 0.6885219620806831\n",
      "|--- Train Loss: 0.6883430090443842\n",
      "|--- Train Loss: 0.6882030288378398\n",
      "|--- Train Loss: 0.6882898576797978\n",
      "|--- Train Loss: 0.6877720039337873\n",
      "|--- Train Loss: 0.6881355769706495\n",
      "|--- Train Loss: 0.6882046776659348\n",
      "|--- Train Loss: 0.6880881292479378\n",
      "|--- Train Loss: 0.6882537371582456\n",
      "|--- Train Loss: 0.688261362346443\n",
      "|--- Train Loss: 0.6880157809508475\n",
      "|--- Train Loss: 0.6883680010453249\n",
      "|--- Train Loss: 0.6888038292527199\n",
      "|--- Train Loss: 0.6886122473856298\n",
      "|--- Train Loss: 0.6890184297448113\n",
      "|--- Train Loss: 0.6892103358756664\n",
      "|--- Train Loss: 0.6895270564339377\n",
      "|--- Train Loss: 0.689227384991116\n",
      "|--- Train Loss: 0.6896312612554302\n",
      "|--- Train Loss: 0.6898577796651962\n",
      "|--- Train Loss: 0.6896864511072636\n",
      "|--- Train Loss: 0.6896098329096424\n",
      "|--- Train Loss: 0.6897566044330596\n",
      "|--- Train Loss: 0.6896005413111519\n",
      "|--- Train Loss: 0.6898327893935717\n",
      "|--- Train Loss: 0.6898149611814967\n",
      "|--- Train Loss: 0.6897087660100725\n",
      "|--- Train Loss: 0.6894899975169789\n",
      "|--- Train Loss: 0.6892454837049756\n",
      "|--- Train Loss: 0.6892098983128866\n",
      "|--- Train Loss: 0.6893339290701109\n",
      "|--- Train Loss: 0.6890931573964781\n",
      "|--- Train Loss: 0.6893640081087749\n",
      "|--- Train Loss: 0.6893959915051695\n",
      "|--- Train Loss: 0.6892966032028198\n",
      "|--- Train Loss: 0.6894357705873156\n",
      "|--- Train Loss: 0.6894189510494471\n",
      "|--- Train Loss: 0.6892667366908147\n",
      "|--- Train Loss: 0.689219408866131\n",
      "|--- Train Loss: 0.6892578548459865\n",
      "|--- Train Loss: 0.6890035113867592\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2/10 \n",
      "|--- Train Loss: 0.7177280783653259\n",
      "|--- Train Loss: 0.6996554434299469\n",
      "|--- Train Loss: 0.6946486234664917\n",
      "|--- Train Loss: 0.6932514756917953\n",
      "|--- Train Loss: 0.6909501314163208\n",
      "|--- Train Loss: 0.692380964756012\n",
      "|--- Train Loss: 0.6920176233564105\n",
      "|--- Train Loss: 0.6920523941516876\n",
      "|--- Train Loss: 0.6916549934281243\n",
      "|--- Train Loss: 0.6916141927242279\n",
      "|--- Train Loss: 0.6908311789686029\n",
      "|--- Train Loss: 0.6909751147031784\n",
      "|--- Train Loss: 0.6912760000962478\n",
      "|--- Train Loss: 0.690774645124163\n",
      "|--- Train Loss: 0.6892092744509379\n",
      "|--- Train Loss: 0.6888313218951225\n",
      "|--- Train Loss: 0.6875746355337256\n",
      "|--- Train Loss: 0.6878642870320214\n",
      "|--- Train Loss: 0.6875581741333008\n",
      "|--- Train Loss: 0.6872619688510895\n",
      "|--- Train Loss: 0.6870011516979763\n",
      "|--- Train Loss: 0.6862489418549971\n",
      "|--- Train Loss: 0.6852646625560262\n",
      "|--- Train Loss: 0.684945230682691\n",
      "|--- Train Loss: 0.6848129200935363\n",
      "|--- Train Loss: 0.6856988668441772\n",
      "|--- Train Loss: 0.6857176621754965\n",
      "|--- Train Loss: 0.6863854633910316\n",
      "|--- Train Loss: 0.6864306782854015\n",
      "|--- Train Loss: 0.6868028601010641\n",
      "|--- Train Loss: 0.6873406710163239\n",
      "|--- Train Loss: 0.6872517727315426\n",
      "|--- Train Loss: 0.6870531855207501\n",
      "|--- Train Loss: 0.6867929048397962\n",
      "|--- Train Loss: 0.686753921849387\n",
      "|--- Train Loss: 0.6868269079261355\n",
      "|--- Train Loss: 0.6870177307644406\n",
      "|--- Train Loss: 0.686820503912474\n",
      "|--- Train Loss: 0.686903987175379\n",
      "|--- Train Loss: 0.686879763007164\n",
      "|--- Train Loss: 0.6869973627532401\n",
      "|--- Train Loss: 0.6869676765941438\n",
      "|--- Train Loss: 0.6866303155588549\n",
      "|--- Train Loss: 0.6867690601132133\n",
      "|--- Train Loss: 0.6868586209085252\n",
      "|--- Train Loss: 0.68674679942753\n",
      "|--- Train Loss: 0.6869826481697408\n",
      "|--- Train Loss: 0.6870281361043453\n",
      "|--- Train Loss: 0.6866729162177261\n",
      "|--- Train Loss: 0.6865783417224884\n",
      "|--- Train Loss: 0.6864925809935027\n",
      "|--- Train Loss: 0.6867424582059567\n",
      "|--- Train Loss: 0.6869403438748054\n",
      "|--- Train Loss: 0.6870316090407195\n",
      "|--- Train Loss: 0.6874073299494656\n",
      "|--- Train Loss: 0.6873916408845356\n",
      "|--- Train Loss: 0.687300653834092\n",
      "|--- Train Loss: 0.6875187621034425\n",
      "|--- Train Loss: 0.6876681123749685\n",
      "|--- Train Loss: 0.6880249301592509\n",
      "|--- Train Loss: 0.6881555611969995\n",
      "|--- Train Loss: 0.6883246994787647\n",
      "|--- Train Loss: 0.6885771618949043\n",
      "|--- Train Loss: 0.6885188492015004\n",
      "|--- Train Loss: 0.6888224024039048\n",
      "|--- Train Loss: 0.6890066717610215\n",
      "|--- Train Loss: 0.6891081893621985\n",
      "|--- Train Loss: 0.6891641213613398\n",
      "Epoch: 3/10 \n",
      "|--- Train Loss: 0.6890471577644348\n",
      "|--- Train Loss: 0.6961307227611542\n",
      "|--- Train Loss: 0.695853074391683\n",
      "|--- Train Loss: 0.6976982802152634\n",
      "|--- Train Loss: 0.6949870586395264\n",
      "|--- Train Loss: 0.6944757103919983\n",
      "|--- Train Loss: 0.691636221749442\n",
      "|--- Train Loss: 0.689890131354332\n",
      "|--- Train Loss: 0.6908086140950521\n",
      "|--- Train Loss: 0.6915443956851959\n",
      "|--- Train Loss: 0.6923032132062045\n",
      "|--- Train Loss: 0.6923614740371704\n",
      "|--- Train Loss: 0.6914442823483393\n",
      "|--- Train Loss: 0.6906411520072392\n",
      "|--- Train Loss: 0.6913278579711915\n",
      "|--- Train Loss: 0.6909587383270264\n",
      "|--- Train Loss: 0.6916655933155733\n",
      "|--- Train Loss: 0.691272603140937\n",
      "|--- Train Loss: 0.6907895709338941\n",
      "|--- Train Loss: 0.6907785683870316\n",
      "|--- Train Loss: 0.6908289052191234\n",
      "|--- Train Loss: 0.6913329606706445\n",
      "|--- Train Loss: 0.6904056771941807\n",
      "|--- Train Loss: 0.6912021636962891\n",
      "|--- Train Loss: 0.6912054061889649\n",
      "|--- Train Loss: 0.69105352805211\n",
      "|--- Train Loss: 0.6914061197528133\n",
      "|--- Train Loss: 0.6913746850831168\n",
      "|--- Train Loss: 0.6910188835242699\n",
      "|--- Train Loss: 0.6907093187173208\n",
      "|--- Train Loss: 0.6908950075026481\n",
      "|--- Train Loss: 0.6907240767031908\n",
      "|--- Train Loss: 0.6908228252873276\n",
      "|--- Train Loss: 0.6903173204730538\n",
      "|--- Train Loss: 0.6906057647296361\n",
      "|--- Train Loss: 0.6906038638618257\n",
      "|--- Train Loss: 0.6902554002968041\n",
      "|--- Train Loss: 0.6907017372156444\n",
      "|--- Train Loss: 0.6907157790966523\n",
      "|--- Train Loss: 0.6903024017810822\n",
      "|--- Train Loss: 0.6901606786541823\n",
      "|--- Train Loss: 0.6900427156970614\n",
      "|--- Train Loss: 0.6901040035624837\n",
      "|--- Train Loss: 0.6901163160800934\n",
      "|--- Train Loss: 0.6901055388980442\n",
      "|--- Train Loss: 0.6899992979091146\n",
      "|--- Train Loss: 0.6897884023950455\n",
      "|--- Train Loss: 0.6895063954095045\n",
      "|--- Train Loss: 0.6894152638863544\n",
      "|--- Train Loss: 0.6893716633319855\n",
      "|--- Train Loss: 0.6895703369495916\n",
      "|--- Train Loss: 0.6897951100881283\n",
      "|--- Train Loss: 0.6898016311087698\n",
      "|--- Train Loss: 0.689951135052575\n",
      "|--- Train Loss: 0.6900581164793534\n",
      "|--- Train Loss: 0.6900826767086983\n",
      "|--- Train Loss: 0.6899560156621432\n",
      "|--- Train Loss: 0.6898932981080023\n",
      "|--- Train Loss: 0.6898086646855888\n",
      "|--- Train Loss: 0.689261257648468\n",
      "|--- Train Loss: 0.6891452435587273\n",
      "|--- Train Loss: 0.6892638235322891\n",
      "|--- Train Loss: 0.6892764066892957\n",
      "|--- Train Loss: 0.6888902056962252\n",
      "|--- Train Loss: 0.6889715313911438\n",
      "|--- Train Loss: 0.689050600384221\n",
      "|--- Train Loss: 0.6892418772427004\n",
      "|--- Train Loss: 0.6890781039700788\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4/10 \n",
      "|--- Train Loss: 0.7025402188301086\n",
      "|--- Train Loss: 0.6954203844070435\n",
      "|--- Train Loss: 0.6910619735717773\n",
      "|--- Train Loss: 0.6892671585083008\n",
      "|--- Train Loss: 0.6920344352722168\n",
      "|--- Train Loss: 0.6914913356304169\n",
      "|--- Train Loss: 0.6909716725349426\n",
      "|--- Train Loss: 0.6905213445425034\n",
      "|--- Train Loss: 0.6905439959632026\n",
      "|--- Train Loss: 0.6898968160152436\n",
      "|--- Train Loss: 0.6907050338658419\n",
      "|--- Train Loss: 0.6911846150954565\n",
      "|--- Train Loss: 0.6928307505754324\n",
      "|--- Train Loss: 0.6930194965430668\n",
      "|--- Train Loss: 0.6921337207158407\n",
      "|--- Train Loss: 0.6910260729491711\n",
      "|--- Train Loss: 0.6903566963532392\n",
      "|--- Train Loss: 0.6907967494593726\n",
      "|--- Train Loss: 0.6903835848758095\n",
      "|--- Train Loss: 0.6904992252588272\n",
      "|--- Train Loss: 0.690369569119953\n",
      "|--- Train Loss: 0.6899256164377386\n",
      "|--- Train Loss: 0.6899501188941624\n",
      "|--- Train Loss: 0.6894116749366125\n",
      "|--- Train Loss: 0.6893015241622925\n",
      "|--- Train Loss: 0.6889614141904391\n",
      "|--- Train Loss: 0.6888648072878519\n",
      "|--- Train Loss: 0.6887002331869942\n",
      "|--- Train Loss: 0.6886497489337263\n",
      "|--- Train Loss: 0.6890547255674998\n",
      "|--- Train Loss: 0.6889206536354557\n",
      "|--- Train Loss: 0.6888102050870657\n",
      "|--- Train Loss: 0.6888326695471099\n",
      "|--- Train Loss: 0.6888411132728353\n",
      "|--- Train Loss: 0.6884695427758353\n",
      "|--- Train Loss: 0.6882883922921287\n",
      "|--- Train Loss: 0.6885966352514319\n",
      "|--- Train Loss: 0.6886018153868223\n",
      "|--- Train Loss: 0.688366310718732\n",
      "|--- Train Loss: 0.6885329142212868\n",
      "|--- Train Loss: 0.6887675640059681\n",
      "|--- Train Loss: 0.6888794941561562\n",
      "|--- Train Loss: 0.6887064285056536\n",
      "|--- Train Loss: 0.6889703476970847\n",
      "|--- Train Loss: 0.689040372106764\n",
      "|--- Train Loss: 0.6889753406462462\n",
      "|--- Train Loss: 0.6888478994369507\n",
      "|--- Train Loss: 0.689286332577467\n",
      "|--- Train Loss: 0.6893780766701212\n",
      "|--- Train Loss: 0.6892647874355317\n",
      "|--- Train Loss: 0.6894298137403002\n",
      "|--- Train Loss: 0.6895322352647781\n",
      "|--- Train Loss: 0.6894741935550042\n",
      "|--- Train Loss: 0.6896104293840902\n",
      "|--- Train Loss: 0.689580575986342\n",
      "|--- Train Loss: 0.6894890634076936\n",
      "|--- Train Loss: 0.6896788791606301\n",
      "|--- Train Loss: 0.6900425913005039\n",
      "|--- Train Loss: 0.6899983711161856\n",
      "|--- Train Loss: 0.6902874618768692\n",
      "|--- Train Loss: 0.6900935866793648\n",
      "|--- Train Loss: 0.6901666925799462\n",
      "|--- Train Loss: 0.6902047887681022\n",
      "|--- Train Loss: 0.6900050789117813\n",
      "|--- Train Loss: 0.6900112262138953\n",
      "|--- Train Loss: 0.6898297315294092\n",
      "|--- Train Loss: 0.6896432495828885\n",
      "|--- Train Loss: 0.6894334765041575\n",
      "Epoch: 5/10 \n",
      "|--- Train Loss: 0.6868640780448914\n",
      "|--- Train Loss: 0.6794734001159668\n",
      "|--- Train Loss: 0.6832183400789896\n",
      "|--- Train Loss: 0.6842630356550217\n",
      "|--- Train Loss: 0.6844467878341675\n",
      "|--- Train Loss: 0.6881655851999918\n",
      "|--- Train Loss: 0.6871125102043152\n",
      "|--- Train Loss: 0.68700260668993\n",
      "|--- Train Loss: 0.6882820195621915\n",
      "|--- Train Loss: 0.6882272601127625\n",
      "|--- Train Loss: 0.6889118389649824\n",
      "|--- Train Loss: 0.6883914818366369\n",
      "|--- Train Loss: 0.6896904523556049\n",
      "|--- Train Loss: 0.6912961815084729\n",
      "|--- Train Loss: 0.6897169947624207\n",
      "|--- Train Loss: 0.6891008950769901\n",
      "|--- Train Loss: 0.6895040519097272\n",
      "|--- Train Loss: 0.6897873249318864\n",
      "|--- Train Loss: 0.6890398546269065\n",
      "|--- Train Loss: 0.6885025441646576\n",
      "|--- Train Loss: 0.688570621467772\n",
      "|--- Train Loss: 0.6879031251777302\n",
      "|--- Train Loss: 0.6875890648883322\n",
      "|--- Train Loss: 0.6876539662480354\n",
      "|--- Train Loss: 0.6881628632545471\n",
      "|--- Train Loss: 0.6888743249269632\n",
      "|--- Train Loss: 0.6891025348945901\n",
      "|--- Train Loss: 0.6881863985742841\n",
      "|--- Train Loss: 0.6885473440433371\n",
      "|--- Train Loss: 0.6884164313475291\n",
      "|--- Train Loss: 0.6883795338292276\n",
      "|--- Train Loss: 0.6886100508272648\n",
      "|--- Train Loss: 0.6886042555173238\n",
      "|--- Train Loss: 0.6888216488501605\n",
      "|--- Train Loss: 0.6886951071875436\n",
      "|--- Train Loss: 0.6886681699090533\n",
      "|--- Train Loss: 0.6886630976522291\n",
      "|--- Train Loss: 0.6887536033203727\n",
      "|--- Train Loss: 0.6888486727690085\n",
      "|--- Train Loss: 0.6885082602500916\n",
      "|--- Train Loss: 0.6883088859116159\n",
      "|--- Train Loss: 0.6880222161610922\n",
      "|--- Train Loss: 0.6883695513703102\n",
      "|--- Train Loss: 0.6883314780213616\n",
      "|--- Train Loss: 0.688605723116133\n",
      "|--- Train Loss: 0.6887230847192847\n",
      "|--- Train Loss: 0.6886454199222808\n",
      "|--- Train Loss: 0.6885465780893961\n",
      "|--- Train Loss: 0.6889425297172702\n",
      "|--- Train Loss: 0.6889480757713318\n",
      "|--- Train Loss: 0.68895196797801\n",
      "|--- Train Loss: 0.689060661655206\n",
      "|--- Train Loss: 0.6890443147353407\n",
      "|--- Train Loss: 0.6891072160667844\n",
      "|--- Train Loss: 0.6892977812073448\n",
      "|--- Train Loss: 0.6892411070210593\n",
      "|--- Train Loss: 0.689174171079669\n",
      "|--- Train Loss: 0.6890812341509194\n",
      "|--- Train Loss: 0.6890718947022648\n",
      "|--- Train Loss: 0.6890431592861811\n",
      "|--- Train Loss: 0.6891166860940027\n",
      "|--- Train Loss: 0.6891457313491453\n",
      "|--- Train Loss: 0.6890676248641241\n",
      "|--- Train Loss: 0.6891228631138802\n",
      "|--- Train Loss: 0.689042449914492\n",
      "|--- Train Loss: 0.6890417481913711\n",
      "|--- Train Loss: 0.6889942368464683\n",
      "|--- Train Loss: 0.6891948983949774\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 6/10 \n",
      "|--- Train Loss: 0.6858057379722595\n",
      "|--- Train Loss: 0.6839141845703125\n",
      "|--- Train Loss: 0.6832731564839681\n",
      "|--- Train Loss: 0.6835523694753647\n",
      "|--- Train Loss: 0.6825822830200196\n",
      "|--- Train Loss: 0.6831009387969971\n",
      "|--- Train Loss: 0.6840355566569737\n",
      "|--- Train Loss: 0.6824109926819801\n",
      "|--- Train Loss: 0.6848377188046774\n",
      "|--- Train Loss: 0.6843422651290894\n",
      "|--- Train Loss: 0.6832159865986217\n",
      "|--- Train Loss: 0.6834523677825928\n",
      "|--- Train Loss: 0.6835572169377253\n",
      "|--- Train Loss: 0.683258980512619\n",
      "|--- Train Loss: 0.6834496021270752\n",
      "|--- Train Loss: 0.6836792156100273\n",
      "|--- Train Loss: 0.6840580035658443\n",
      "|--- Train Loss: 0.683677981297175\n",
      "|--- Train Loss: 0.6835926959389135\n",
      "|--- Train Loss: 0.6848993301391602\n",
      "|--- Train Loss: 0.6848526426724025\n",
      "|--- Train Loss: 0.6856803812763907\n",
      "|--- Train Loss: 0.685845320639403\n",
      "|--- Train Loss: 0.6853504156072935\n",
      "|--- Train Loss: 0.6859210371971131\n",
      "|--- Train Loss: 0.6859604670451238\n",
      "|--- Train Loss: 0.685858894277502\n",
      "|--- Train Loss: 0.6862234430653709\n",
      "|--- Train Loss: 0.6869031766365314\n",
      "|--- Train Loss: 0.6876437187194824\n",
      "|--- Train Loss: 0.688301722849569\n",
      "|--- Train Loss: 0.6885262280702591\n",
      "|--- Train Loss: 0.6889090267094699\n",
      "|--- Train Loss: 0.6886790201944464\n",
      "|--- Train Loss: 0.6889302321842738\n",
      "|--- Train Loss: 0.6886567506525252\n",
      "|--- Train Loss: 0.6883456449250918\n",
      "|--- Train Loss: 0.6884009085203472\n",
      "|--- Train Loss: 0.6886072617310744\n",
      "|--- Train Loss: 0.6888104051351547\n",
      "|--- Train Loss: 0.688886450558174\n",
      "|--- Train Loss: 0.6889386773109436\n",
      "|--- Train Loss: 0.6889882032261339\n",
      "|--- Train Loss: 0.6891831091859124\n",
      "|--- Train Loss: 0.689111512237125\n",
      "|--- Train Loss: 0.6894677851511084\n",
      "|--- Train Loss: 0.6895873686100574\n",
      "|--- Train Loss: 0.6898615285754204\n",
      "|--- Train Loss: 0.6897453312971153\n",
      "|--- Train Loss: 0.6896777331829071\n",
      "|--- Train Loss: 0.6893857331836925\n",
      "|--- Train Loss: 0.6894193543837621\n",
      "|--- Train Loss: 0.6892242825256204\n",
      "|--- Train Loss: 0.6895122892326779\n",
      "|--- Train Loss: 0.6895898688923229\n",
      "|--- Train Loss: 0.6894240741218839\n",
      "|--- Train Loss: 0.6892513392264383\n",
      "|--- Train Loss: 0.6889168784536165\n",
      "|--- Train Loss: 0.6887125958830623\n",
      "|--- Train Loss: 0.6884268780549367\n",
      "|--- Train Loss: 0.6885812859066197\n",
      "|--- Train Loss: 0.6884592627325365\n",
      "|--- Train Loss: 0.6886324570292518\n",
      "|--- Train Loss: 0.6890725307166576\n",
      "|--- Train Loss: 0.6890362611183753\n",
      "|--- Train Loss: 0.6891067822774252\n",
      "|--- Train Loss: 0.6892362414900937\n",
      "|--- Train Loss: 0.6892117218059652\n",
      "Epoch: 7/10 \n",
      "|--- Train Loss: 0.6809285283088684\n",
      "|--- Train Loss: 0.695779412984848\n",
      "|--- Train Loss: 0.7010651032129923\n",
      "|--- Train Loss: 0.6982865929603577\n",
      "|--- Train Loss: 0.6946652412414551\n",
      "|--- Train Loss: 0.6935877203941345\n",
      "|--- Train Loss: 0.6932310036250523\n",
      "|--- Train Loss: 0.6922762617468834\n",
      "|--- Train Loss: 0.6931826604737176\n",
      "|--- Train Loss: 0.6923103272914887\n",
      "|--- Train Loss: 0.6912820935249329\n",
      "|--- Train Loss: 0.6909785817066828\n",
      "|--- Train Loss: 0.6919874365513141\n",
      "|--- Train Loss: 0.6906728872231075\n",
      "|--- Train Loss: 0.6908862789471945\n",
      "|--- Train Loss: 0.6918459311127663\n",
      "|--- Train Loss: 0.691019850618699\n",
      "|--- Train Loss: 0.69132732351621\n",
      "|--- Train Loss: 0.6913459426478336\n",
      "|--- Train Loss: 0.6915150582790375\n",
      "|--- Train Loss: 0.6916150280407497\n",
      "|--- Train Loss: 0.6910830600695177\n",
      "|--- Train Loss: 0.6904400846232539\n",
      "|--- Train Loss: 0.6898741399248441\n",
      "|--- Train Loss: 0.6907648706436157\n",
      "|--- Train Loss: 0.6910441219806671\n",
      "|--- Train Loss: 0.6914862725469801\n",
      "|--- Train Loss: 0.6914277374744415\n",
      "|--- Train Loss: 0.6914233359797247\n",
      "|--- Train Loss: 0.691944831609726\n",
      "|--- Train Loss: 0.6912320121642082\n",
      "|--- Train Loss: 0.6909967139363289\n",
      "|--- Train Loss: 0.6909971128810536\n",
      "|--- Train Loss: 0.6911997234120089\n",
      "|--- Train Loss: 0.6909588575363159\n",
      "|--- Train Loss: 0.690956261422899\n",
      "|--- Train Loss: 0.6902202883282224\n",
      "|--- Train Loss: 0.6901338806277827\n",
      "|--- Train Loss: 0.6902891947672918\n",
      "|--- Train Loss: 0.6899081543087959\n",
      "|--- Train Loss: 0.6899234347227143\n",
      "|--- Train Loss: 0.6898268560568491\n",
      "|--- Train Loss: 0.6894649904827739\n",
      "|--- Train Loss: 0.6896204257553274\n",
      "|--- Train Loss: 0.6898916575643751\n",
      "|--- Train Loss: 0.6898097369981848\n",
      "|--- Train Loss: 0.6901134163775342\n",
      "|--- Train Loss: 0.6899552531540394\n",
      "|--- Train Loss: 0.6898801424065415\n",
      "|--- Train Loss: 0.689902195930481\n",
      "|--- Train Loss: 0.6898278722576067\n",
      "|--- Train Loss: 0.6894676891657022\n",
      "|--- Train Loss: 0.689680169213493\n",
      "|--- Train Loss: 0.6898366301148026\n",
      "|--- Train Loss: 0.6896475434303284\n",
      "|--- Train Loss: 0.6897204709904534\n",
      "|--- Train Loss: 0.6896282047556158\n",
      "|--- Train Loss: 0.6891867754788235\n",
      "|--- Train Loss: 0.6892180089223183\n",
      "|--- Train Loss: 0.6892660349607468\n",
      "|--- Train Loss: 0.6894576832896373\n",
      "|--- Train Loss: 0.689354823481652\n",
      "|--- Train Loss: 0.6892765741499643\n",
      "|--- Train Loss: 0.6892892112955451\n",
      "|--- Train Loss: 0.68946125965852\n",
      "|--- Train Loss: 0.6892042449026397\n",
      "|--- Train Loss: 0.6893605580970422\n",
      "|--- Train Loss: 0.6889727317235049\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 8/10 \n",
      "|--- Train Loss: 0.6976151466369629\n",
      "|--- Train Loss: 0.6965980231761932\n",
      "|--- Train Loss: 0.6953824957211813\n",
      "|--- Train Loss: 0.6942048668861389\n",
      "|--- Train Loss: 0.6927160024642944\n",
      "|--- Train Loss: 0.6925841470559438\n",
      "|--- Train Loss: 0.6910609347479684\n",
      "|--- Train Loss: 0.6906987130641937\n",
      "|--- Train Loss: 0.6894263625144958\n",
      "|--- Train Loss: 0.6896987378597259\n",
      "|--- Train Loss: 0.6900422085415233\n",
      "|--- Train Loss: 0.6910407890876135\n",
      "|--- Train Loss: 0.6912095592572138\n",
      "|--- Train Loss: 0.6913903781345913\n",
      "|--- Train Loss: 0.6912887096405029\n",
      "|--- Train Loss: 0.6909700855612755\n",
      "|--- Train Loss: 0.6901413026977988\n",
      "|--- Train Loss: 0.6902733213371701\n",
      "|--- Train Loss: 0.6902029953504863\n",
      "|--- Train Loss: 0.6894889891147613\n",
      "|--- Train Loss: 0.6888402303059896\n",
      "|--- Train Loss: 0.6888733614574779\n",
      "|--- Train Loss: 0.6883718526881674\n",
      "|--- Train Loss: 0.687719446917375\n",
      "|--- Train Loss: 0.6881505751609802\n",
      "|--- Train Loss: 0.6886341250859774\n",
      "|--- Train Loss: 0.6884939935472276\n",
      "|--- Train Loss: 0.6884340239422662\n",
      "|--- Train Loss: 0.6885443329811096\n",
      "|--- Train Loss: 0.6888056516647338\n",
      "|--- Train Loss: 0.6891062990311654\n",
      "|--- Train Loss: 0.688908014446497\n",
      "|--- Train Loss: 0.6892165216532621\n",
      "|--- Train Loss: 0.6891453301205355\n",
      "|--- Train Loss: 0.6890344074794225\n",
      "|--- Train Loss: 0.6890072325865427\n",
      "|--- Train Loss: 0.6894996278994793\n",
      "|--- Train Loss: 0.6896764808579495\n",
      "|--- Train Loss: 0.6895781877713326\n",
      "|--- Train Loss: 0.6897013455629348\n",
      "|--- Train Loss: 0.6901147191117449\n",
      "|--- Train Loss: 0.6899982520512172\n",
      "|--- Train Loss: 0.6900350285130877\n",
      "|--- Train Loss: 0.6897949346087195\n",
      "|--- Train Loss: 0.6900098972850376\n",
      "|--- Train Loss: 0.6900750489338584\n",
      "|--- Train Loss: 0.6900282286583109\n",
      "|--- Train Loss: 0.6898068934679031\n",
      "|--- Train Loss: 0.6897557876547988\n",
      "|--- Train Loss: 0.6895455253124237\n",
      "|--- Train Loss: 0.6894782606293174\n",
      "|--- Train Loss: 0.6895460337400436\n",
      "|--- Train Loss: 0.6899181548154579\n",
      "|--- Train Loss: 0.6897958627453556\n",
      "|--- Train Loss: 0.6897989511489868\n",
      "|--- Train Loss: 0.6897395359618324\n",
      "|--- Train Loss: 0.6893273058690523\n",
      "|--- Train Loss: 0.6891847520039\n",
      "|--- Train Loss: 0.689207281096507\n",
      "|--- Train Loss: 0.6892834722995758\n",
      "|--- Train Loss: 0.6893469564250259\n",
      "|--- Train Loss: 0.6891494672144612\n",
      "|--- Train Loss: 0.688931511508094\n",
      "|--- Train Loss: 0.6889396496117115\n",
      "|--- Train Loss: 0.6889042340792142\n",
      "|--- Train Loss: 0.6888312020085074\n",
      "|--- Train Loss: 0.6887995075823655\n",
      "|--- Train Loss: 0.6887476260171217\n",
      "Epoch: 9/10 \n",
      "|--- Train Loss: 0.6832144260406494\n",
      "|--- Train Loss: 0.6742216348648071\n",
      "|--- Train Loss: 0.6795565287272135\n",
      "|--- Train Loss: 0.6802475154399872\n",
      "|--- Train Loss: 0.6830885410308838\n",
      "|--- Train Loss: 0.6825059950351715\n",
      "|--- Train Loss: 0.6811464514051165\n",
      "|--- Train Loss: 0.6809753924608231\n",
      "|--- Train Loss: 0.6810193856557211\n",
      "|--- Train Loss: 0.6835349202156067\n",
      "|--- Train Loss: 0.684902695092288\n",
      "|--- Train Loss: 0.6843089858690897\n",
      "|--- Train Loss: 0.683121749987969\n",
      "|--- Train Loss: 0.6839161855833871\n",
      "|--- Train Loss: 0.6838243285814921\n",
      "|--- Train Loss: 0.6841235160827637\n",
      "|--- Train Loss: 0.6846916289890513\n",
      "|--- Train Loss: 0.6845152278741201\n",
      "|--- Train Loss: 0.6844812474752727\n",
      "|--- Train Loss: 0.6841192483901978\n",
      "|--- Train Loss: 0.6845231112979707\n",
      "|--- Train Loss: 0.6844301169568842\n",
      "|--- Train Loss: 0.6853059711663619\n",
      "|--- Train Loss: 0.685290701687336\n",
      "|--- Train Loss: 0.6852752423286438\n",
      "|--- Train Loss: 0.6860688626766205\n",
      "|--- Train Loss: 0.6864012435630515\n",
      "|--- Train Loss: 0.6867796246494565\n",
      "|--- Train Loss: 0.6875467300415039\n",
      "|--- Train Loss: 0.6872664590676626\n",
      "|--- Train Loss: 0.6873240932341544\n",
      "|--- Train Loss: 0.6877401564270258\n",
      "|--- Train Loss: 0.6881339929320596\n",
      "|--- Train Loss: 0.6875838444513434\n",
      "|--- Train Loss: 0.6875241875648499\n",
      "|--- Train Loss: 0.6879535284307268\n",
      "|--- Train Loss: 0.6878222871471096\n",
      "|--- Train Loss: 0.688573170649378\n",
      "|--- Train Loss: 0.6887149184178083\n",
      "|--- Train Loss: 0.6889288321137428\n",
      "|--- Train Loss: 0.6885684757697873\n",
      "|--- Train Loss: 0.6887450487840743\n",
      "|--- Train Loss: 0.6889115932375885\n",
      "|--- Train Loss: 0.6888483600182966\n",
      "|--- Train Loss: 0.6890170852343241\n",
      "|--- Train Loss: 0.6891989448796147\n",
      "|--- Train Loss: 0.6892392559254423\n",
      "|--- Train Loss: 0.6890702222784361\n",
      "|--- Train Loss: 0.6895972478146456\n",
      "|--- Train Loss: 0.6895300948619842\n",
      "|--- Train Loss: 0.6894455633911432\n",
      "|--- Train Loss: 0.6892779985299478\n",
      "|--- Train Loss: 0.6894040096480891\n",
      "|--- Train Loss: 0.6891264970655795\n",
      "|--- Train Loss: 0.6893913269042968\n",
      "|--- Train Loss: 0.6896042472549847\n",
      "|--- Train Loss: 0.6894661480920357\n",
      "|--- Train Loss: 0.689321376126388\n",
      "|--- Train Loss: 0.6893474116163739\n",
      "|--- Train Loss: 0.6892417500416438\n",
      "|--- Train Loss: 0.6890885898324309\n",
      "|--- Train Loss: 0.6889393271938447\n",
      "|--- Train Loss: 0.6887974663386269\n",
      "|--- Train Loss: 0.6888271579518914\n",
      "|--- Train Loss: 0.6888731562174284\n",
      "|--- Train Loss: 0.6888813990535159\n",
      "|--- Train Loss: 0.6888243831805329\n",
      "|--- Train Loss: 0.688508028493208\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 10/10 \n",
      "|--- Train Loss: 0.6881091594696045\n",
      "|--- Train Loss: 0.6878008246421814\n",
      "|--- Train Loss: 0.6857540607452393\n",
      "|--- Train Loss: 0.6872181445360184\n",
      "|--- Train Loss: 0.6892216444015503\n",
      "|--- Train Loss: 0.6878904104232788\n",
      "|--- Train Loss: 0.6899257302284241\n",
      "|--- Train Loss: 0.6887084916234016\n",
      "|--- Train Loss: 0.6889627708329095\n",
      "|--- Train Loss: 0.6889150738716125\n",
      "|--- Train Loss: 0.6888617819005792\n",
      "|--- Train Loss: 0.6879333903392156\n",
      "|--- Train Loss: 0.6880524708674505\n",
      "|--- Train Loss: 0.6886884825570243\n",
      "|--- Train Loss: 0.6890057643254598\n",
      "|--- Train Loss: 0.6886491738259792\n",
      "|--- Train Loss: 0.6885475270888385\n",
      "|--- Train Loss: 0.6881509257687463\n",
      "|--- Train Loss: 0.6883400615892912\n",
      "|--- Train Loss: 0.6883024603128434\n",
      "|--- Train Loss: 0.6885444692202977\n",
      "|--- Train Loss: 0.6888773089105432\n",
      "|--- Train Loss: 0.6890548338060793\n",
      "|--- Train Loss: 0.6898361866672834\n",
      "|--- Train Loss: 0.6891232490539551\n",
      "|--- Train Loss: 0.6888767114052405\n",
      "|--- Train Loss: 0.689095053407881\n",
      "|--- Train Loss: 0.6895517302410943\n",
      "|--- Train Loss: 0.6902200230236711\n",
      "|--- Train Loss: 0.6901183108488719\n",
      "|--- Train Loss: 0.6896639793149887\n",
      "|--- Train Loss: 0.6895765457302332\n",
      "|--- Train Loss: 0.6898826053648284\n",
      "|--- Train Loss: 0.6898106704739964\n",
      "|--- Train Loss: 0.689660234110696\n",
      "|--- Train Loss: 0.6896486779054006\n",
      "|--- Train Loss: 0.6894276641510628\n",
      "|--- Train Loss: 0.6896983149804568\n",
      "|--- Train Loss: 0.6896431232110048\n",
      "|--- Train Loss: 0.6894294351339341\n",
      "|--- Train Loss: 0.6892159115977403\n",
      "|--- Train Loss: 0.6894163021019527\n",
      "|--- Train Loss: 0.6897598685220231\n",
      "|--- Train Loss: 0.6898540664802898\n",
      "|--- Train Loss: 0.6900442467795478\n",
      "|--- Train Loss: 0.6898891835109048\n",
      "|--- Train Loss: 0.6898904404741653\n",
      "|--- Train Loss: 0.6899836199978987\n",
      "|--- Train Loss: 0.6900003175346219\n",
      "|--- Train Loss: 0.689893651008606\n",
      "|--- Train Loss: 0.6897058007763881\n",
      "|--- Train Loss: 0.689741334089866\n",
      "|--- Train Loss: 0.6897370151753696\n",
      "|--- Train Loss: 0.6898845886742627\n",
      "|--- Train Loss: 0.6897610263390974\n",
      "|--- Train Loss: 0.6897194609045982\n",
      "|--- Train Loss: 0.6896566936844274\n",
      "|--- Train Loss: 0.689384678314472\n",
      "|--- Train Loss: 0.6893679439011267\n",
      "|--- Train Loss: 0.6895019928614299\n",
      "|--- Train Loss: 0.6895134312207581\n",
      "|--- Train Loss: 0.6894400312054542\n",
      "|--- Train Loss: 0.6896530749305846\n",
      "|--- Train Loss: 0.6893701376393437\n",
      "|--- Train Loss: 0.6892470919168913\n",
      "|--- Train Loss: 0.6890652577082316\n",
      "|--- Train Loss: 0.6892883074817373\n",
      "|--- Train Loss: 0.6892124502097859\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    print('Epoch: {}/{} '.format(epoch,epochs))\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    try:\n",
    "        for idx, (X, y) in enumerate(train_data):\n",
    "    #         if X is not None and y is not None or (X,y) is not None:\n",
    "            X, y = X, y\n",
    "\n",
    "\n",
    "            opt.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "\n",
    "                output = model(X)\n",
    "\n",
    "                loss = loss_fn1(output, y)\n",
    "\n",
    "                # loss.type(torch.FloatTensor)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            print('|--- Train Loss: {}'.format(np.mean(train_loss)))\n",
    "        \n",
    "        if(epoch % 2 == 0):\n",
    "            val_loss = []\n",
    "            val_acc = []\n",
    "            with torch.no_grad():\n",
    "\n",
    "                    model.eval()\n",
    "                    for id, (X_val, y_val) in enumerate(validation_data):\n",
    "                        X_val, y_val = X_val, y_val\n",
    "\n",
    "                        output = model(X_val)\n",
    "                        loss1 = loss_fn1(output, y_val)\n",
    "                        val_loss.append(loss1.item())\n",
    "                        _, y_pred = torch.max(output, dim=1)\n",
    "                        val_acc.append(accuracy_score(y_val.numpy(), y_pred.numpy()))\n",
    "\n",
    "                    print('|--- Val Loss: {} --- Val Accuracy: {}'.format(np.mean(val_loss), np.mean(val_acc)))\n",
    "                # print('F1 score for validation: %f'%(np.mean(epoch_val_score)))\n",
    "\n",
    "        print('-'*80)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:36:25.304840Z",
     "iopub.status.busy": "2023-05-14T13:36:25.304335Z",
     "iopub.status.idle": "2023-05-14T13:36:25.940624Z",
     "shell.execute_reply": "2023-05-14T13:36:25.939446Z",
     "shell.execute_reply.started": "2023-05-14T13:36:25.304805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- Test Loss: 0.6942233219742775 --- Test Accuracy: 0.5297475961538461 --- F1 Score: 0.6127028793693678\n",
      "Confusion matrix:\n",
      " [[19 27]\n",
      " [23 35]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.41      0.43        46\n",
      "           1       0.56      0.60      0.58        58\n",
      "\n",
      "    accuracy                           0.52       104\n",
      "   macro avg       0.51      0.51      0.51       104\n",
      "weighted avg       0.51      0.52      0.52       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ttloss = []\n",
    "ttacc = []\n",
    "ttf1 = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for id, (X_val, y_val) in enumerate(test_data):\n",
    "        X_val, y_val = X_val, y_val\n",
    "\n",
    "        output = model(X_val)\n",
    "        loss1 = loss_fn1(output, y_val)\n",
    "        ttloss.append(loss1.item())\n",
    "        _, y_pred = torch.max(output, dim=1)\n",
    "        y_val, y_pred = y_val.numpy(), y_pred.numpy()\n",
    "        ttacc.append(accuracy_score(y_val, y_pred))\n",
    "        ttf1.append(f1_score(y_val, y_pred))\n",
    "        res = confusion_matrix(y_val, y_pred)\n",
    "        report = classification_report(y_val, y_pred)\n",
    "\n",
    "    print('|--- Test Loss: {} --- Test Accuracy: {} --- F1 Score: {}'.format(np.mean(ttloss), np.mean(ttacc), np.mean(ttf1)))\n",
    "    print('Confusion matrix:\\n',res)\n",
    "    print('Report:\\n',report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:18:44.408773Z",
     "iopub.status.busy": "2023-05-14T13:18:44.407606Z",
     "iopub.status.idle": "2023-05-14T13:18:44.439845Z",
     "shell.execute_reply": "2023-05-14T13:18:44.438314Z",
     "shell.execute_reply.started": "2023-05-14T13:18:44.408710Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model,'/kaggle/working/model_IMD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations \n",
    "\n",
    "# https://github.com/Torak28/Image-manipulation-detection\n",
    "# https://github.com/z1311/Image-Manipulation-Detection\n",
    "# https://paperswithcode.com/task/image-manipulation-detection\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
